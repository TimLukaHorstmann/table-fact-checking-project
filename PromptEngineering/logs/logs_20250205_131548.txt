2025-02-05 13:15:48,093 - INFO - Command used to run the script: prompt_engineering.py --repo_folder ../original_repo --csv_folder data/all_csv/ --dataset tokenized_data/test_examples.json --learning_type chain_of_thought --format_type markdown,naturalized,json,html --models mistral:latest,phi4:latest,llama3.2:latest --parallel_models --batch_prompts --max_workers 10
2025-02-05 13:15:48,160 - INFO - Loaded data from ../original_repo/tokenized_data/test_examples.json.
2025-02-05 13:15:48,160 - INFO - Loaded data from ../original_repo/tokenized_data/test_examples.json.
2025-02-05 13:15:48,164 - INFO - Loaded data from ../original_repo/tokenized_data/test_examples.json.
2025-02-05 13:15:48,164 - INFO - Loaded data from ../original_repo/tokenized_data/test_examples.json.
2025-02-05 13:15:48,164 - INFO - Loaded data from ../original_repo/tokenized_data/test_examples.json.
2025-02-05 13:15:48,164 - INFO - Loaded data from ../original_repo/tokenized_data/test_examples.json.
2025-02-05 13:15:48,164 - INFO - Loaded data from ../original_repo/tokenized_data/test_examples.json.
2025-02-05 13:15:48,164 - INFO - Loaded data from ../original_repo/tokenized_data/test_examples.json.
2025-02-05 13:15:48,164 - INFO - Loaded data from ../original_repo/tokenized_data/test_examples.json.
2025-02-05 13:15:48,164 - INFO - Loaded data from ../original_repo/tokenized_data/test_examples.json.
2025-02-05 13:15:48,164 - INFO - Loaded data from ../original_repo/tokenized_data/test_examples.json.
2025-02-05 13:15:48,164 - INFO - Loaded data from ../original_repo/tokenized_data/test_examples.json.
2025-02-05 13:15:48,218 - INFO - Ollama started successfully (or was already running).
2025-02-05 13:15:48,218 - INFO - Ollama started successfully (or was already running).
2025-02-05 13:15:48,219 - INFO - Processing 1695 tables in parallel (skipping 0 already checkpointed) out of 1695 remaining.
2025-02-05 13:15:48,219 - INFO - Processing 1695 tables in parallel (skipping 0 already checkpointed) out of 1695 remaining.
2025-02-05 13:15:48,248 - INFO - Ollama started successfully (or was already running).
2025-02-05 13:15:48,248 - INFO - Ollama started successfully (or was already running).
2025-02-05 13:15:48,248 - INFO - Processing 1695 tables in parallel (skipping 0 already checkpointed) out of 1695 remaining.
2025-02-05 13:15:48,249 - INFO - Ollama started successfully (or was already running).
2025-02-05 13:15:48,249 - INFO - Ollama started successfully (or was already running).
2025-02-05 13:15:48,249 - INFO - Ollama started successfully (or was already running).
2025-02-05 13:15:48,249 - INFO - Ollama started successfully (or was already running).
2025-02-05 13:15:48,249 - INFO - Ollama started successfully (or was already running).
2025-02-05 13:15:48,249 - INFO - Ollama started successfully (or was already running).
2025-02-05 13:15:48,249 - INFO - Ollama started successfully (or was already running).
2025-02-05 13:15:48,249 - INFO - Ollama started successfully (or was already running).
2025-02-05 13:15:48,249 - INFO - Processing 1695 tables in parallel (skipping 0 already checkpointed) out of 1695 remaining.
2025-02-05 13:15:48,250 - INFO - Processing 1695 tables in parallel (skipping 0 already checkpointed) out of 1695 remaining.
2025-02-05 13:15:48,250 - INFO - Processing 1695 tables in parallel (skipping 0 already checkpointed) out of 1695 remaining.
2025-02-05 13:15:48,250 - INFO - Processing 1695 tables in parallel (skipping 0 already checkpointed) out of 1695 remaining.
2025-02-05 13:15:48,250 - INFO - Processing 1695 tables in parallel (skipping 0 already checkpointed) out of 1695 remaining.
2025-02-05 13:15:48,251 - INFO - Processing 1695 tables in parallel (skipping 0 already checkpointed) out of 1695 remaining.
2025-02-05 13:15:48,251 - INFO - Processing 1695 tables in parallel (skipping 0 already checkpointed) out of 1695 remaining.
2025-02-05 13:15:48,251 - INFO - Processing 1695 tables in parallel (skipping 0 already checkpointed) out of 1695 remaining.
2025-02-05 13:15:48,252 - INFO - Processing 1695 tables in parallel (skipping 0 already checkpointed) out of 1695 remaining.
2025-02-05 13:15:49,754 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate "HTTP/1.1 500 Internal Server Error"
2025-02-05 13:15:49,755 - ERROR - Error invoking LLM: llama runner process has terminated: signal: broken pipe
2025-02-05 13:15:55,217 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate "HTTP/1.1 500 Internal Server Error"
2025-02-05 13:15:55,218 - ERROR - Error invoking LLM: llama runner process has terminated: signal: broken pipe
2025-02-05 13:16:11,393 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate "HTTP/1.1 500 Internal Server Error"
2025-02-05 13:16:11,393 - ERROR - Error invoking LLM: llama runner process has terminated: signal: broken pipe
2025-02-05 13:16:16,850 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate "HTTP/1.1 500 Internal Server Error"
2025-02-05 13:16:16,851 - ERROR - Error invoking LLM: llama runner process has terminated: signal: broken pipe
2025-02-05 13:16:17,254 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate "HTTP/1.1 500 Internal Server Error"
2025-02-05 13:16:17,254 - ERROR - Error invoking LLM: llama runner process has terminated: signal: broken pipe
2025-02-05 13:16:22,431 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate "HTTP/1.1 500 Internal Server Error"
2025-02-05 13:16:22,431 - ERROR - Error invoking LLM: llama runner process has terminated: signal: broken pipe
2025-02-05 13:16:27,527 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate "HTTP/1.1 500 Internal Server Error"
2025-02-05 13:16:27,528 - ERROR - Error invoking LLM: llama runner process has terminated: signal: broken pipe
2025-02-05 13:16:32,649 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate "HTTP/1.1 500 Internal Server Error"
2025-02-05 13:16:32,649 - ERROR - Error invoking LLM: llama runner process has terminated: signal: broken pipe
2025-02-05 13:16:37,726 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate "HTTP/1.1 500 Internal Server Error"
2025-02-05 13:16:37,726 - ERROR - Error invoking LLM: llama runner process has terminated: signal: broken pipe
2025-02-05 13:16:42,826 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate "HTTP/1.1 500 Internal Server Error"
2025-02-05 13:16:42,826 - ERROR - Error invoking LLM: llama runner process has terminated: signal: broken pipe
2025-02-05 13:16:47,936 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate "HTTP/1.1 500 Internal Server Error"
2025-02-05 13:16:47,936 - ERROR - Error invoking LLM: llama runner process has terminated: signal: broken pipe
2025-02-05 13:16:53,036 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate "HTTP/1.1 500 Internal Server Error"
2025-02-05 13:16:53,036 - ERROR - Error invoking LLM: llama runner process has terminated: signal: broken pipe
2025-02-05 13:16:58,126 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate "HTTP/1.1 500 Internal Server Error"
2025-02-05 13:16:58,126 - ERROR - Error invoking LLM: llama runner process has terminated: signal: broken pipe
2025-02-05 13:17:03,232 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate "HTTP/1.1 500 Internal Server Error"
2025-02-05 13:17:03,233 - ERROR - Error invoking LLM: llama runner process has terminated: signal: broken pipe
2025-02-05 13:17:08,383 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate "HTTP/1.1 500 Internal Server Error"
2025-02-05 13:17:08,383 - ERROR - Error invoking LLM: llama runner process has terminated: signal: broken pipe
2025-02-05 13:17:13,489 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate "HTTP/1.1 500 Internal Server Error"
2025-02-05 13:17:13,489 - ERROR - Error invoking LLM: llama runner process has terminated: signal: broken pipe
2025-02-05 13:17:18,615 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate "HTTP/1.1 500 Internal Server Error"
2025-02-05 13:17:18,616 - ERROR - Error invoking LLM: llama runner process has terminated: signal: broken pipe
2025-02-05 13:17:23,690 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate "HTTP/1.1 500 Internal Server Error"
2025-02-05 13:17:23,691 - ERROR - Error invoking LLM: llama runner process has terminated: signal: broken pipe
2025-02-05 13:17:28,788 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate "HTTP/1.1 500 Internal Server Error"
2025-02-05 13:17:28,789 - ERROR - Error invoking LLM: llama runner process has terminated: signal: broken pipe
2025-02-05 13:17:33,885 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate "HTTP/1.1 500 Internal Server Error"
2025-02-05 13:17:33,886 - ERROR - Error invoking LLM: llama runner process has terminated: signal: broken pipe
2025-02-05 13:17:38,984 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate "HTTP/1.1 500 Internal Server Error"
2025-02-05 13:17:38,985 - ERROR - Error invoking LLM: llama runner process has terminated: signal: broken pipe
2025-02-05 13:17:44,079 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate "HTTP/1.1 500 Internal Server Error"
2025-02-05 13:17:44,080 - ERROR - Error invoking LLM: llama runner process has terminated: signal: broken pipe
2025-02-05 13:17:49,185 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate "HTTP/1.1 500 Internal Server Error"
2025-02-05 13:17:49,185 - ERROR - Error invoking LLM: llama runner process has terminated: signal: broken pipe
2025-02-05 13:17:54,297 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate "HTTP/1.1 500 Internal Server Error"
2025-02-05 13:17:54,297 - ERROR - Error invoking LLM: llama runner process has terminated: signal: broken pipe
2025-02-05 13:17:59,392 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate "HTTP/1.1 500 Internal Server Error"
2025-02-05 13:17:59,393 - ERROR - Error invoking LLM: llama runner process has terminated: signal: broken pipe
2025-02-05 13:18:04,493 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate "HTTP/1.1 500 Internal Server Error"
2025-02-05 13:18:04,493 - ERROR - Error invoking LLM: llama runner process has terminated: signal: broken pipe
2025-02-05 13:18:09,583 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate "HTTP/1.1 500 Internal Server Error"
2025-02-05 13:18:09,584 - ERROR - Error invoking LLM: llama runner process has terminated: signal: broken pipe
2025-02-05 13:18:11,547 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,548 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,548 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,548 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,548 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,548 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,548 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,548 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,548 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,548 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,548 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,548 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,548 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,548 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,548 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,548 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,549 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,549 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,549 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,549 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,549 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,549 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,549 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,549 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,549 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,549 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,549 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,549 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,549 - ERROR - Error invoking LLM: Server disconnected without sending a response.
2025-02-05 13:18:11,550 - ERROR - Error invoking LLM: Server disconnected without sending a response.
