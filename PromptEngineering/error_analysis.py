#!/miniconda3/envs/llm/bin/python
"""
error_analysis.py

This script performs error analysis on result JSON files generated by any of the approaches.
It uses the shared analyze_errors() function from factchecker_base.
"""

import os
import argparse
import json
import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import display
import numpy as np
import glob
import re
from tqdm.notebook import tqdm
from langchain_ollama import OllamaLLM
import logging

def load_failed_predictions(root_dir: str, pattern: str = "results_with_cells_*.json") -> list:
    """
    Scan the given root_dir (and its subdirectories) for files matching the pattern "results_with_cells_*.json",
    and return a list of dictionaries for the predictions where the model failed (i.e., predicted_response != true_response).
    """
    failed_predictions = []
    successful_predictions = []

    # Use glob to find all matching files, recursively in subdirectories
    print("Searching in directory:", os.path.abspath(root_dir))
    file_list = glob.glob(os.path.join(root_dir, '**', pattern), recursive=True)
    if not file_list:
        print("No intermediate results files found matching pattern.")
        return failed_predictions, successful_predictions
    
    # Initialize a counter for the IDs
    id_counter = 1

    for file_path in tqdm(file_list):
        try:
            with open(file_path, "r") as f:
                results = json.load(f)
        except Exception as e:
            print(f"Could not load {file_path}: {e}")
            continue

        for result in results:
            # Add an ID to each claim
            result["ID"] = id_counter
            id_counter += 1  # Increment the ID counter

            # Check if the model's predicted response is different from the true response
            if result.get("predicted_response") != result.get("true_response"):
                # Create a dictionary with relevant details for failed predictions
                failed_prediction = {
                    "ID": result["ID"],
                    "predicted_response": result["predicted_response"],
                    "true_response": result["true_response"],
                    "claim": result["claim"]
                }
                failed_predictions.append(failed_prediction)
            else:
                # Create a dictionary with relevant details for successful predictions
                successful_prediction = {
                    "ID": result["ID"],
                    "predicted_response": result["predicted_response"],
                    "true_response": result["true_response"],
                    "claim": result["claim"]
                }
                successful_predictions.append(successful_prediction)
    
    return failed_predictions, successful_predictions

# from factchecker_base import analyze_errors

def template_generation(category: str, category_explanation: str, claim: str) -> str:
    """
    Generates a template for the LLM that explains the task and asks for the categorization decision.
    
    Arguments:
    - category_explanation: A detailed explanation of the category, providing context and guidance.
    - claim: The claim that needs to be categorized.
    - category: The specific category for which the claim is being assessed.
    
    Returns:
    - A string that serves as the prompt for the LLM.
    """
    
    # Start building the prompt
    prompt = f"""
    You are an expert in categorizing claims based on specific criteria. Your task is to determine whether a claim falls under a particular category.
    
    {category_explanation}

    Your role is to decide whether the following claim falls under the category: "{category}".

    Claim: "{claim}"

    Provide a clear and objective reasoning for your decision. Think about the specific characteristics of this category and the claim's content. Be honest and impartial in your evaluation.

    Then, at the end, answer with either 'TRUE' or 'FALSE' based on whether the claim fits this category.

    Reasoning:
    """
    
    return prompt

def aggregation_prompt_generation(claim: str) -> str:
    """
    Generates a prompt for the LLM to categorize a claim as falling under the 'aggregation' category.
    
    Arguments:
    - claim: The claim to be assessed for aggregation.
    
    Returns:
    - A string that serves as the prompt for the LLM to evaluate the claim.
    """
    
    # Explanation for aggregation category
    category_explanation = """
    Aggregation refers to operations where data is combined or summarized into a total or an average.
    If the claim involves a sum, an average, a total or such an operation to be verified, it falls under this category.
    """

    # Generate the prompt using the template_generation function
    return template_generation(category_explanation, claim, "aggregation")

def negate_prompt_generation(claim: str) -> str:
    """
    Generates a prompt for the LLM to categorize a claim as falling under the 'negation' category.
    
    Arguments:
    - claim: The claim to be assessed for negation.
    
    Returns:
    - A string that serves as the prompt for the LLM to evaluate the claim.
    """
    
    # Refined explanation for negation category
    category_explanation = """
    Negation refers to sentences that deny or contradict a claim or assertion. 
    These often include phrases such as "did not", "never", or "not", which indicate that the statement is asserting the absence or opposite of something. 
    """
    
    # Generate the prompt using the template_generation function
    return template_generation(category_explanation, claim, "negate")

def superlative_prompt_generation(claim: str) -> str:
    """
    Generates a prompt for the LLM to categorize a claim as falling under the 'superlative' category.
    
    Arguments:
    - claim: The claim to be assessed for superlative.
    
    Returns:
    - A string that serves as the prompt for the LLM to evaluate the claim.
    """
    
    # Refined explanation for superlative category
    category_explanation = """
    Superlative refers to sentences that express the highest degree or extreme of something. 
    This often involves words like "best", "most", "highest", "greatest", which indicate that the claim is about the extreme or top value in a given context. 
    """
    
    # Generate the prompt using the template_generation function
    return template_generation(category_explanation, claim, "superlative")

def count_prompt_generation(claim: str) -> str:
    """
    Generates a prompt for the 'count' category. 
    Count refers to claims that mention specific counts of objects, actions, or values.
    
    Args:
    - claim : The claim data containing the ID and the claim text.

    Returns:
    - str: The generated prompt for the 'count' category.
    """
    category_explanation = """
    Count refers to claims that involve specific numbers or quantities of objects, actions, or values.
    For example, “there are 5 countries in the tournament” or “xxx achieved 3 goals” are examples of claims involving counts, where the number of items or events is being verified.
    """
    
    return template_generation(category="count", category_explanation=category_explanation, claim=claim["claim"])


def comparative_prompt_generation(claim: str) -> str:
    """
    Generates a prompt for the LLM to categorize a claim as falling under the 'comparative' category.
    
    Arguments:
    - claim: The claim to be assessed for comparative.
    
    Returns:
    - A string that serves as the prompt for the LLM to evaluate the claim.
    """
    
    # Refined explanation for comparative category
    category_explanation = """
    Comparative refers to claims that make comparisons between two or more items or concepts. 
    These claims often include words like "better", "more", "greater", "worse", "less", "higher", etc., to indicate a comparison of relative values or qualities between different entities. 
    """
    
    # Generate the prompt using the template_generation function
    return template_generation(category_explanation, claim, "comparative")

def ordinal_prompt_generation(claim: str) -> str:
    """
    Generates a prompt for the LLM to categorize a claim as falling under the 'ordinal' category.
    
    Arguments:
    - claim: The claim to be assessed for ordinal.
    
    Returns:
    - A string that serves as the prompt for the LLM to evaluate the claim.
    """
    
    # Refined explanation for ordinal category
    category_explanation = """
    Ordinal refers to claims that involve rankings or positions in a sequence. These claims typically involve terms like "first", "second", "last", "top", "bottom", "ranked", and so on, to indicate a specific position or order in a list.
    """
    
    # Generate the prompt using the template_generation function
    return template_generation(category_explanation, claim, "ordinal")


def unique_prompt_generation(claim: str) -> str:
    """
    Generates a prompt for the LLM to categorize a claim as falling under the 'unique' category.
    
    Arguments:
    - claim: The claim to be assessed for uniqueness.
    
    Returns:
    - A string that serves as the prompt for the LLM to evaluate the claim.
    """
    
    # Refined explanation for unique category
    category_explanation = """
    Unique refers to claims that involve the distinctness or uniqueness of items or entities. These claims often use phrases like "different", "no two", "only one", "none alike", and "distinct". The claim may suggest that something is the only one of its kind or that no two items are the same in some context.
    For example, "There are 5 different nations in the tournament" or "There are no two players from the U.S." would fall under this category.
    """
    
    # Generate the prompt using the template_generation function
    return template_generation(category_explanation, claim, "unique")


def all_prompt_generation(claim: str) -> str:
    """
    Generates a prompt for the LLM to categorize a claim as falling under the 'all' category.
    
    Arguments:
    - claim: The claim to be assessed for being universally applicable.
    
    Returns:
    - A string that serves as the prompt for the LLM to evaluate the claim.
    """
    
    # Refined explanation for all category
    category_explanation = """
    The 'All' category refers to claims that involve statements about the entirety or totality of a set or group. These claims use phrases like "all of", "every", "none of", or "always". The claim might refer to the full set or group, with no exceptions or exclusions.
    For example, "All of the trains are departing in the morning" or "None of the people are older than 25" would fall under this category.
    """
    
    # Generate the prompt using the template_generation function
    return template_generation(category_explanation, claim, "all")


def none_prompt_generation(claim: dict) -> str:
    """
    Generates a prompt for the LLM to categorize a claim as falling under the 'none' category.
    
    Arguments:
    - claim: The claim to be assessed for being a simple factual statement.
    
    Returns:
    - A string that serves as the prompt for the LLM to evaluate the claim.
    """
    
    # Refined explanation for none category
    category_explanation = """
    The 'None' category refers to claims that are simple statements without any complex operations such as sums, comparisons, or qualifications. These claims typically involve basic factual information about a subject, like a specific value or identity.
    
    For example, "Player X achieves 2 points in Game Y" or "Player X is from Country Z" would fall under this category as they simply state facts without involving higher-order operations.
    """
    
    # Generate the prompt using the template_generation function
    return template_generation(category_explanation, claim, "none")

# Define the error analysis pipeline function
def error_analysis_pipeline(failed_predictions, successful_predictions, model: OllamaLLM):
    """
    Runs error analysis over failed and successful predictions, processing claims through various prompt generation functions.
    """
    all_claims = failed_predictions + successful_predictions  # Combine both lists of claims
    
    # Define the categories and their associated prompt generation functions
    prompt_functions_with_categories = {
        "aggregation_prompt_generation": "aggregation",
        "negate_prompt_generation": "negate",
        "superlative_prompt_generation": "superlative",
        "count_prompt_generation": "count",
        "comparative_prompt_generation": "comparative",
        "ordinal_prompt_generation": "ordinal",
        "unique_prompt_generation": "unique",
        "all_prompt_generation": "all",
        "none_prompt_generation": "none"
    }
    
    # Iterate over each prompt generation function
    for prompt_function, category in prompt_functions_with_categories.items():
        logging.info(f"Processing prompt function: {prompt_function}")

        # Iterate over all claims for the current prompt function
        for prediction in all_claims:
            try:
                # Dynamically call the prompt generation function
                prompt_generation_func = globals().get(prompt_function)
                
                if prompt_generation_func:
                    # Generate the prompt for the current claim
                    claim = prediction['claim']
                    prompt = prompt_generation_func(claim)
                    logging.info(f"Generated prompt for claim: {claim}")

                    # Get model prediction for the generated prompt
                    model_response = re.search(r"\bTRUE\b", model.invoke(prompt).strip())
                    prediction_copy = prediction.copy()  # Copy the claim to avoid modifying the original
                    prediction_copy["model_response"] = model_response

                    if model_response == "true":
                        # If model response is "true", add the category key-value pair
                        prediction_copy[category] = True  # Add the category key with a True value

                    # Update the correct list based on whether the claim was successful or failed
                    prediction_id = prediction_copy["ID"]
                    if prediction_copy["predicted_response"] != prediction_copy["true_response"]:
                        # The claim was a failure, update in failed_predictions
                        for i, prediction in enumerate(failed_predictions):
                            if prediction["ID"] == prediction_id:
                                failed_predictions[i] = prediction_copy
                                break
                    else:
                        # The claim was successful, update in successful_predictions
                        for i, prediction in enumerate(successful_predictions):
                            if prediction["ID"] == prediction_id:
                                successful_predictions[i] = prediction_copy
                                break
                else:
                    logging.error(f"Prompt function {prompt_function} not found.")
            except Exception as e:
                claim = prediction['claim']
                logging.error(f"Error processing claim {claim} with {prompt_function}: {e}")
                prediction_copy = prediction.copy()
                prediction_copy["model_response"] = None

                # Update the correct list based on whether the claim was successful or failed
                claim_id = prediction_copy["ID"]
                if prediction_copy["predicted_response"] != prediction_copy["true_response"]:
                    # The claim was a failure, update in failed_predictions
                    for i, claim in enumerate(failed_predictions):
                        if claim["ID"] == claim_id:
                            failed_predictions[i] = prediction_copy
                            break
                else:
                    # The claim was successful, update in successful_predictions
                    for i, claim in enumerate(successful_predictions):
                        if claim["ID"] == claim_id:
                            successful_predictions[i] = prediction_copy
                            break

    # Return the updated failed_predictions and successful_predictions lists
    return failed_predictions, successful_predictions

# Example usage (you would call this function from your code with the right data and model):
# results = error_analysis_pipeline(failed_predictions, successful_predictions, model)


def plot_classification_results(failed_predictions, successful_predictions):
    """
    Plots a bar plot showing the percentage of correct and incorrect classifications
    per category (aggregation, negate, superlative, etc.) based on failed and successful predictions.
    
    Args:
    - failed_predictions (list): List of dictionaries representing failed predictions.
    - successful_predictions (list): List of dictionaries representing successful predictions.
    """
    # Combine both lists of predictions
    all_predictions = failed_predictions + successful_predictions
    
    # Initialize a dictionary to count correct and incorrect classifications for each category
    categories = ["aggregation", "negate", "superlative", "count", "comparative", "ordinal", "unique", "all", "none"]
    category_results = {category: {"correct": 0, "incorrect": 0} for category in categories}
    
    # Loop through all predictions to count correct and incorrect classifications for each category
    for prediction in all_predictions:
        # Check if the prediction was correct
        true_label = prediction["true_response"]
        predicted_label = prediction["predicted_response"]

        # If true_label and predicted_label match, it's a correct classification
        is_correct = true_label == predicted_label

        # Loop through categories and check which categories apply to this prediction
        for category in categories:
            if category in prediction and prediction[category]:  # If the category is in the prediction and set to True
                if is_correct:
                    category_results[category]["correct"] += 1
                else:
                    category_results[category]["incorrect"] += 1

    # Calculate the percentages for each category
    correct_percentages = [100 * category_results[category]["correct"] / len(all_predictions) for category in categories]
    incorrect_percentages = [100 * category_results[category]["incorrect"] / len(all_predictions) for category in categories]

    # Plotting the bar plot
    bar_width = 0.4
    index = np.arange(len(categories))
    
    fig, ax = plt.subplots(figsize=(10, 6))
    
    ax.bar(index, correct_percentages, bar_width, label='Correct', color='g')
    ax.bar(index, incorrect_percentages, bar_width, bottom=correct_percentages, label='Incorrect', color='r')

    ax.set_xlabel('Categories')
    ax.set_ylabel('Percentage of Claims (%)')
    ax.set_title('Correct vs Incorrect Classifications per Category')
    ax.set_xticks(index)
    ax.set_xticklabels(categories, rotation=45)
    ax.legend()
    
    # Show the plot
    plt.tight_layout()
    plt.show()




docs_results_folder = "docs/results"
failed_predictions, successful_predictions = load_failed_predictions(docs_results_folder)
model = OllamaLLM(model="mistral-8b")


failed_predictions, successful_predictions = error_analysis_pipeline(failed_predictions, successful_predictions, model)

plot_classification_results(failed_predictions, successful_predictions)

# for f in failed_predictions:
#     print(f['claim'])
#     print()

# def main():
#     parser = argparse.ArgumentParser(description="Run Error Analysis on a Result JSON File")
#     parser.add_argument("--result_file", type=str, required=True, help="Path to the result JSON file")
#     args = parser.parse_args()
#     analyze_errors(args.result_file)

# if __name__ == "__main__":
#     main()
