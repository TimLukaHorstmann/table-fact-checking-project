#!/miniconda3/envs/llm/bin/python
"""
error_analysis.py

This script performs error analysis on result JSON files generated by any of the approaches.
It uses the shared analyze_errors() function from factchecker_base.
"""

from logging import config
import os
import argparse
import json
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import glob
import re
from tqdm import tqdm
from langchain_ollama import OllamaLLM
import logging
import concurrent.futures

################################################################################
#                        LOAD CLAIMS
################################################################################

import json

def load_claims(file_path: str) -> list:
    """
    Load a JSON file from the given path and return a list of dictionaries with 'claim' and 'ID' 
    for each element.
    """
    all_claims = []
    
    try:
        with open(file_path, 'r') as f:
            data = json.load(f)
        
        id_counter = 1
        
        for item in data:
            claim_with_resp = {
                "ID": id_counter,
                "claim": item.get("claim", ""),
            }
            
            id_counter += 1
            
            all_claims.append(claim_with_resp)
    
    except Exception as e:
        print(f"Error loading file: {e}")
    
    return all_claims

################################################################################
#                        PROMPT GENERATORS FOR CLASSIFICATION
################################################################################

def template_generation(category_explanation: str, claim: str, category: str) -> str:
    """
    Generates a template for the LLM that explains the task and asks for the categorization decision.
    
    Arguments:
    - category_explanation: A detailed explanation of the category, providing context and guidance.
    - claim: The claim that needs to be categorized.
    - category: The specific category for which the claim is being assessed.
    
    Returns:
    - A string that serves as the prompt for the LLM.
    """
    
    # Start building the prompt
    prompt = f"""
    You are an expert in categorizing claims based on specific criteria. Your task is to determine whether a claim falls under a particular category.
    
    {category_explanation}

    Your role is to decide whether the following claim falls under the category: "{category}".

    Claim: "{claim}"

    Provide a clear and objective reasoning for your decision. Think about the specific characteristics of this category and the claim's content. Be honest and impartial in your evaluation.

    Then, at the end, answer with either 'TRUE' or 'FALSE' based on whether the claim fits this category.

    Reasoning:
    """
    
    return prompt

def aggregation_prompt_generation(claim: str) -> str:
    """
    Generates a prompt for the LLM to categorize a claim as falling under the 'aggregation' category.
    
    Arguments:
    - claim: The claim to be assessed for aggregation.
    
    Returns:
    - A string that serves as the prompt for the LLM to evaluate the claim.
    """
    
    # Explanation for aggregation category
    category_explanation = """
    Aggregation refers to operations where data is combined or summarized into a total or an average.
    If the claim involves a sum, an average, a total or such an operation to be verified, it falls under this category.
    """

    # Generate the prompt using the template_generation function
    return template_generation(category_explanation, claim, "aggregation")

def negate_prompt_generation(claim: str) -> str:
    """
    Generates a prompt for the LLM to categorize a claim as falling under the 'negation' category.
    
    Arguments:
    - claim: The claim to be assessed for negation.
    
    Returns:
    - A string that serves as the prompt for the LLM to evaluate the claim.
    """
    
    # Refined explanation for negation category
    category_explanation = """
    Negation refers to sentences that deny or contradict a claim or assertion. 
    These often include phrases such as "did not", "never", or "not", which indicate that the statement is asserting the absence or opposite of something. 
    """
    
    # Generate the prompt using the template_generation function
    return template_generation(category_explanation, claim, "negate")

def superlative_prompt_generation(claim: str) -> str:
    """
    Generates a prompt for the LLM to categorize a claim as falling under the 'superlative' category.
    
    Arguments:
    - claim: The claim to be assessed for superlative.
    
    Returns:
    - A string that serves as the prompt for the LLM to evaluate the claim.
    """
    
    # Refined explanation for superlative category
    category_explanation = """
    Superlative refers to sentences that express the highest degree or extreme of something. 
    This often involves words like "best", "most", "highest", "greatest", which indicate that the claim is about the extreme or top value in a given context. 
    """
    
    # Generate the prompt using the template_generation function
    return template_generation(category_explanation, claim, "superlative")

def count_prompt_generation(claim: str) -> str:
    """
    Generates a prompt for the 'count' category. 
    Count refers to claims that mention specific counts of objects, actions, or values.
    
    Args:
    - claim : The claim data containing the ID and the claim text.

    Returns:
    - str: The generated prompt for the 'count' category.
    """
    category_explanation = """
    Count refers to claims that involve specific numbers or quantities of objects, actions, or values.
    For example, “there are 5 countries in the tournament” or “xxx achieved 3 goals” are examples of claims involving counts, where the number of items or events is being verified.
    """
    
    return template_generation(category_explanation, claim, "count")

def comparative_prompt_generation(claim: str) -> str:
    """
    Generates a prompt for the LLM to categorize a claim as falling under the 'comparative' category.
    
    Arguments:
    - claim: The claim to be assessed for comparative.
    
    Returns:
    - A string that serves as the prompt for the LLM to evaluate the claim.
    """
    
    # Refined explanation for comparative category
    category_explanation = """
    Comparative refers to claims that make comparisons between two or more items or concepts. 
    These claims often include words like "better", "more", "greater", "worse", "less", "higher", etc., to indicate a comparison of relative values or qualities between different entities. 
    """
    
    # Generate the prompt using the template_generation function
    return template_generation(category_explanation, claim, "comparative")

def ordinal_prompt_generation(claim: str) -> str:
    """
    Generates a prompt for the LLM to categorize a claim as falling under the 'ordinal' category.
    
    Arguments:
    - claim: The claim to be assessed for ordinal.
    
    Returns:
    - A string that serves as the prompt for the LLM to evaluate the claim.
    """
    
    # Refined explanation for ordinal category
    category_explanation = """
    Ordinal refers to claims that involve rankings or positions in a sequence. These claims typically involve terms like "first", "second", "last", "top", "bottom", "ranked", and so on, to indicate a specific position or order in a list.
    """
    
    # Generate the prompt using the template_generation function
    return template_generation(category_explanation, claim, "ordinal")

def unique_prompt_generation(claim: str) -> str:
    """
    Generates a prompt for the LLM to categorize a claim as falling under the 'unique' category.
    
    Arguments:
    - claim: The claim to be assessed for uniqueness.
    
    Returns:
    - A string that serves as the prompt for the LLM to evaluate the claim.
    """
    
    # Refined explanation for unique category
    category_explanation = """
    Unique refers to claims that involve the distinctness or uniqueness of items or entities. These claims often use phrases like "different", "no two", "only one", "none alike", and "distinct". The claim may suggest that something is the only one of its kind or that no two items are the same in some context.
    For example, "There are 5 different nations in the tournament" or "There are no two players from the U.S." would fall under this category.
    """
    
    # Generate the prompt using the template_generation function
    return template_generation(category_explanation, claim, "unique")

def all_prompt_generation(claim: str) -> str:
    """
    Generates a prompt for the LLM to categorize a claim as falling under the 'all' category.
    
    Arguments:
    - claim: The claim to be assessed for being universally applicable.
    
    Returns:
    - A string that serves as the prompt for the LLM to evaluate the claim.
    """
    
    # Refined explanation for all category
    category_explanation = """
    The 'All' category refers to claims that involve statements about the entirety or totality of a set or group. These claims use phrases like "all of", "every", "none of", or "always". The claim might refer to the full set or group, with no exceptions or exclusions.
    For example, "All of the trains are departing in the morning" or "None of the people are older than 25" would fall under this category.
    """
    
    # Generate the prompt using the template_generation function
    return template_generation(category_explanation, claim, "all")

def none_prompt_generation(claim: str) -> str:
    """
    Generates a prompt for the LLM to categorize a claim as falling under the 'none' category.
    
    Arguments:
    - claim: The claim to be assessed for being a simple factual statement.
    
    Returns:
    - A string that serves as the prompt for the LLM to evaluate the claim.
    """
    
    # Refined explanation for none category
    category_explanation = """
    The 'None' category refers to claims that are simple statements without any complex operations such as sums, comparisons, or qualifications. These claims typically involve basic factual information about a subject, like a specific value or identity.
    
    For example, "Player X achieves 2 points in Game Y" or "Player X is from Country Z" would fall under this category as they simply state facts without involving higher-order operations.
    """
    
    # Generate the prompt using the template_generation function
    return template_generation(category_explanation, claim, "none")


################################################################################
#                        CATEGORISE CLAIMS
################################################################################

def process_prediction(all_claims, i, prompt_generation_function, category, model):
    try:
        
        claim = all_claims[i]['claim']
        prompt = prompt_generation_function(claim)

        model_response = model.invoke(prompt)
        model_true = re.search(r"\bTRUE\b", model_response)

        print(model_response)
        print("model_true:", model_true)

        if model_true:
            print("ohouiiiii")
            all_claims[i][category] = True

    except Exception as e:
        claim = all_claims[i]['claim']
        # logging.error(f"Error processing claim {claim}: {e}")

def error_analysis_pipeline(all_claims, model_name, prompt_functions_with_categories):
    """
    Runs error analysis over failed and successful predictions, processing claims through various prompt generation functions.
    """
    try:
        # Initialize the model
        if "deepseek" in model_name:
            model = OllamaLLM(model=model_name, params={"n_ctx": 4096, "n_batch": 256})
        else:
            model = OllamaLLM(model=model_name)
    except Exception as e:
        # logging.error(f"Failed to initialize model {model_name}: {e}")
        return
    
    # Iterate through each prompt function and its associated category
    for prompt_function_name, category in prompt_functions_with_categories.items():
        print(f"Processing {prompt_function_name}")

        prompt_generation_function = globals().get(prompt_function_name)
        if not prompt_generation_function:
            # logging.error(f"Prompt generation function {prompt_function_name} not found.")
            continue  # Skip to the next prompt function if not found

        # Use ThreadPoolExecutor for parallel processing of claims
        with concurrent.futures.ThreadPoolExecutor() as executor:
            futures = []
            total_claims = len(all_claims)

            # Create a tqdm progress bar for claims processing
            with tqdm(total=total_claims, desc=f"Processing {category} claims", ncols=100, position=0) as pbar:
                # Submit tasks to the executor for processing each claim
                for i in range(total_claims):
                    futures.append(executor.submit(process_prediction, all_claims, i, prompt_generation_function, category, model))
                    
                # Wait for all futures to complete and update the progress bar
                for future in concurrent.futures.as_completed(futures):
                    # try:
                    future.result()  # Get result of each future, ensuring completion
                    # except Exception as e:
                        # logging.error(f"Error processing future: {e}")
                    pbar.update(1)  # Update the progress bar

    return all_claims



################################################################################
#                        SAVE AND LOAD CLAIMS
################################################################################

def save_to_json(data, file_path):
    """
    Save a list of dictionaries to a JSON file.
    """
    try:
        with open(file_path, "w") as json_file:
            json.dump(data, json_file, indent=2)
        print(f"Data saved to {file_path}")
    except Exception as e:
        print(f"Error saving data to {file_path}: {e}")

def load_json(file_path):
    """
    Load a JSON file containing a list of dictionaries.
    """
    try:
        with open(file_path, "r") as json_file:
            data = json.load(json_file)
        return data
    except Exception as e:
        print(f"Error loading JSON file: {e}")
        return []


################################################################################
#                        PLOT CATEGORISED CLAIMS
################################################################################

def plot_classification_results(failed_predictions, successful_predictions):
    """
    Plots a bar plot showing the percentage of correct and incorrect classifications
    per category (aggregation, negate, superlative, etc.) based on failed and successful predictions.
    
    Args:
    - failed_predictions (list): List of dictionaries representing failed predictions.
    - successful_predictions (list): List of dictionaries representing successful predictions.
    """
    # Combine both lists of predictions
    all_predictions = failed_predictions + successful_predictions
    
    # Initialize a dictionary to count correct and incorrect classifications for each category
    categories = ["aggregation", "negate", "superlative", "count", "comparative", "ordinal", "unique", "all", "none"]
    category_results = {category: {"correct": 0, "incorrect": 0} for category in categories}
    
    # Loop through all predictions to count correct and incorrect classifications for each category
    for prediction in all_predictions:
        # Check if the prediction was correct
        true_label = prediction["true_response"]
        predicted_label = prediction["predicted_response"]

        # If true_label and predicted_label match, it's a correct classification
        is_correct = true_label == predicted_label

        # Loop through categories and check which categories apply to this prediction
        for category in categories:
            if category in prediction and prediction[category]:  # If the category is in the prediction and set to True
                if is_correct:
                    category_results[category]["correct"] += 1
                else:
                    category_results[category]["incorrect"] += 1

    # Calculate the percentages for each category
    correct_percentages = [100 * category_results[category]["correct"] / len(all_predictions) for category in categories]
    incorrect_percentages = [100 * category_results[category]["incorrect"] / len(all_predictions) for category in categories]

    # Plotting the bar plot
    bar_width = 0.4
    index = np.arange(len(categories))
    
    fig, ax = plt.subplots(figsize=(10, 6))
    
    ax.bar(index, correct_percentages, bar_width, label='Correct', color='g')
    ax.bar(index, incorrect_percentages, bar_width, bottom=correct_percentages, label='Incorrect', color='r')

    ax.set_xlabel('Categories')
    ax.set_ylabel('Percentage of Claims (%)')
    ax.set_title('Correct vs Incorrect Classifications per Category')
    ax.set_xticks(index)
    ax.set_xticklabels(categories, rotation=45)
    ax.legend()
    
    # Show the plot
    plt.tight_layout()
    plt.show()




################################################################################
#                        MAIN
################################################################################

docs_results_folder = "docs/results_test"
all_claims = load_claims(docs_results_folder)

model_name = "mistral:latest"
prompt_functions_with_categories = {
    "aggregation_prompt_generation": "aggregation",
    "negate_prompt_generation": "negate",
    "superlative_prompt_generation": "superlative",
    "count_prompt_generation": "count",
    "comparative_prompt_generation": "comparative",
    "ordinal_prompt_generation": "ordinal",
    "unique_prompt_generation": "unique",
    "all_prompt_generation": "all",
    "none_prompt_generation": "none"
}

categorised_claims = error_analysis_pipeline(all_claims, model_name, prompt_functions_with_categories)
save_to_json("PromptEngineering/categorised_claims.json", categorised_claims)

# for f in failed_predictions:
#     print(f['claim'])
#     print()

# def main():
#     parser = argparse.ArgumentParser(description="Run Error Analysis on a Result JSON File")
#     parser.add_argument("--result_file", type=str, required=True, help="Path to the result JSON file")
#     args = parser.parse_args()
#     analyze_errors(args.result_file)

# if __name__ == "__main__":
#     main()